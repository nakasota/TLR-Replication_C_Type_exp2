#!/usr/bin/env python3
"""
content_validatedç‰ˆã¨merged_onlyç‰ˆã®Ground Truthæ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¤‰æ›´å†…å®¹æ¤œè¨¼ã®åŠ¹æœã‚’è©³ç´°ã«åˆ†æã™ã‚‹
"""

import json
import os
from pathlib import Path
from collections import defaultdict, Counter

def load_ground_truth_data(file_path):
    """Ground Truthãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    if not os.path.exists(file_path):
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return None
    
    with open(file_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèªã—ã¦é©åˆ‡ã«å‡¦ç†
    if isinstance(raw_data, dict) and 'ground_truth' in raw_data:
        return raw_data['ground_truth']
    elif isinstance(raw_data, list):
        return raw_data
    else:
        print(f"âŒ ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼: {file_path}")
        return None

def analyze_ground_truth_quality(data, version_name):
    """Ground Truthå“è³ªã‚’åˆ†æ"""
    print(f"\nğŸ“Š {version_name} ã®è©³ç´°åˆ†æ:")
    
    if not data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return {}
    
    total_proposals = len(data)
    total_files = sum(len(item.get('files', [])) for item in data)
    total_functions = sum(len(item.get('detected_functions', [])) for item in data)
    
    # ææ¡ˆåˆ¥çµ±è¨ˆ
    proposal_stats = []
    content_validation_count = 0
    validation_status_counts = Counter()
    
    for item in data:
        proposal_id = item.get('proposal_id', 'unknown')
        files_count = len(item.get('files', []))
        functions_count = len(item.get('detected_functions', []))
        
        # content_validationæƒ…å ±ã‚’åˆ†æ
        functions_with_validation = 0
        for func in item.get('detected_functions', []):
            if 'content_validation' in func:
                functions_with_validation += 1
                content_validation_count += 1
                validation_status = func['content_validation'].get('validation_status', 'unknown')
                validation_status_counts[validation_status] += 1
        
        proposal_stats.append({
            'proposal_id': proposal_id,
            'files_count': files_count,
            'functions_count': functions_count,
            'functions_with_validation': functions_with_validation
        })
    
    # åŸºæœ¬çµ±è¨ˆ
    print(f"â”œâ”€ ææ¡ˆæ•°: {total_proposals}")
    print(f"â”œâ”€ ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
    print(f"â”œâ”€ ç·é–¢æ•°æ•°: {total_functions}")
    print(f"â”œâ”€ å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°/ææ¡ˆ: {total_files/total_proposals:.2f}")
    print(f"â”œâ”€ å¹³å‡é–¢æ•°æ•°/ææ¡ˆ: {total_functions/total_proposals:.2f}")
    print(f"â”œâ”€ å¤‰æ›´å†…å®¹æ¤œè¨¼æ¸ˆã¿é–¢æ•°: {content_validation_count}")
    
    if validation_status_counts:
        print(f"â””â”€ æ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å†…è¨³:")
        for status, count in validation_status_counts.most_common():
            print(f"   â””â”€ {status}: {count}é–¢æ•°")
    
    return {
        'total_proposals': total_proposals,
        'total_files': total_files,
        'total_functions': total_functions,
        'proposal_stats': proposal_stats,
        'content_validation_count': content_validation_count,
        'validation_status_counts': validation_status_counts,
        'data': data
    }

def compare_ground_truth_impact(merged_only_data, content_validated_data):
    """å¤‰æ›´å†…å®¹æ¤œè¨¼ã®å½±éŸ¿ã‚’æ¯”è¼ƒ"""
    print(f"\nğŸ” å¤‰æ›´å†…å®¹æ¤œè¨¼ã®å½±éŸ¿åˆ†æ:")
    
    # ææ¡ˆIDã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸ã‚’ä½œæˆ
    merged_only_dict = {item['proposal_id']: item for item in merged_only_data['data']}
    content_validated_dict = {item['proposal_id']: item for item in content_validated_data['data']}
    
    # ææ¡ˆã®å·®åˆ†
    merged_only_proposals = set(merged_only_dict.keys())
    content_validated_proposals = set(content_validated_dict.keys())
    
    removed_proposals = merged_only_proposals - content_validated_proposals
    remaining_proposals = merged_only_proposals & content_validated_proposals
    
    print(f"â”œâ”€ merged_onlyã®ææ¡ˆæ•°: {len(merged_only_proposals)}")
    print(f"â”œâ”€ content_validatedã®ææ¡ˆæ•°: {len(content_validated_proposals)}")
    print(f"â”œâ”€ é™¤å¤–ã•ã‚ŒãŸææ¡ˆ: {len(removed_proposals)}")
    print(f"â””â”€ æ®‹å­˜ã—ãŸææ¡ˆ: {len(remaining_proposals)}")
    
    # é™¤å¤–ã•ã‚ŒãŸææ¡ˆã®è©³ç´°
    if removed_proposals:
        print(f"\nâŒ å¤‰æ›´å†…å®¹æ¤œè¨¼ã§é™¤å¤–ã•ã‚ŒãŸææ¡ˆ:")
        for proposal_id in sorted(removed_proposals):
            item = merged_only_dict[proposal_id]
            files_count = len(item.get('files', []))
            functions_count = len(item.get('detected_functions', []))
            print(f"   â””â”€ {proposal_id}: {files_count}ãƒ•ã‚¡ã‚¤ãƒ«, {functions_count}é–¢æ•°")
    
    # æ®‹ã£ãŸææ¡ˆã§ã®é–¢æ•°æ•°ã®å¤‰åŒ–
    function_reductions = []
    total_functions_removed = 0
    
    for proposal_id in remaining_proposals:
        merged_item = merged_only_dict[proposal_id]
        validated_item = content_validated_dict[proposal_id]
        
        merged_functions = len(merged_item.get('detected_functions', []))
        validated_functions = len(validated_item.get('detected_functions', []))
        function_diff = validated_functions - merged_functions
        
        if function_diff < 0:
            function_reductions.append({
                'proposal_id': proposal_id,
                'function_diff': function_diff,
                'merged_functions': merged_functions,
                'validated_functions': validated_functions
            })
            total_functions_removed += abs(function_diff)
    
    print(f"\nğŸ“‰ é–¢æ•°æ•°ãŒæ¸›å°‘ã—ãŸææ¡ˆ: {len(function_reductions)}")
    print(f"ğŸ“Š ç·é™¤å¤–é–¢æ•°æ•°: {total_functions_removed}")
    
    if function_reductions:
        print(f"\nğŸ”¬ é–¢æ•°æ•°æ¸›å°‘ã®è©³ç´°ï¼ˆä¸Šä½10ä»¶ï¼‰:")
        for reduction in sorted(function_reductions, key=lambda x: x['function_diff'])[:10]:
            print(f"   â””â”€ {reduction['proposal_id']}: "
                  f"{reduction['merged_functions']}â†’{reduction['validated_functions']} "
                  f"({reduction['function_diff']} é–¢æ•°é™¤å¤–)")
    
    return {
        'removed_proposals': removed_proposals,
        'remaining_proposals': remaining_proposals,
        'function_reductions': function_reductions,
        'total_functions_removed': total_functions_removed
    }

def analyze_validation_quality(content_validated_data):
    """æ¤œè¨¼å“è³ªã®è©³ç´°åˆ†æ"""
    print(f"\nğŸ¯ å¤‰æ›´å†…å®¹æ¤œè¨¼å“è³ªã®è©³ç´°:")
    
    match_scores = []
    validation_details = Counter()
    
    for proposal in content_validated_data['data']:
        for func in proposal.get('detected_functions', []):
            if 'content_validation' in func:
                validation = func['content_validation']
                score = validation.get('content_match_score', 0.0)
                match_scores.append(score)
                
                status = validation.get('validation_status', 'unknown')
                validation_details[status] += 1
    
    if match_scores:
        avg_score = sum(match_scores) / len(match_scores)
        high_quality = sum(1 for score in match_scores if score >= 0.8)
        medium_quality = sum(1 for score in match_scores if 0.5 <= score < 0.8)
        low_quality = sum(1 for score in match_scores if score < 0.5)
        
        print(f"â”œâ”€ å¹³å‡ä¸€è‡´ã‚¹ã‚³ã‚¢: {avg_score:.3f}")
        print(f"â”œâ”€ é«˜å“è³ªé–¢æ•° (â‰¥0.8): {high_quality}")
        print(f"â”œâ”€ ä¸­å“è³ªé–¢æ•° (0.5-0.8): {medium_quality}")
        print(f"â”œâ”€ ä½å“è³ªé–¢æ•° (<0.5): {low_quality}")
        print(f"â””â”€ ç·æ¤œè¨¼æ¸ˆã¿é–¢æ•°: {len(match_scores)}")
    
    return {
        'avg_score': avg_score if match_scores else 0,
        'match_scores': match_scores,
        'validation_details': validation_details
    }

def save_comparison_report(analysis_results, output_path):
    """æ¯”è¼ƒçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    report = {
        'merged_only_stats': {
            'total_proposals': analysis_results['merged_only']['total_proposals'],
            'total_files': analysis_results['merged_only']['total_files'],
            'total_functions': analysis_results['merged_only']['total_functions']
        },
        'content_validated_stats': {
            'total_proposals': analysis_results['content_validated']['total_proposals'],
            'total_files': analysis_results['content_validated']['total_files'],
            'total_functions': analysis_results['content_validated']['total_functions']
        },
        'validation_impact': {
            'removed_proposals_count': len(analysis_results['comparison']['removed_proposals']),
            'removed_proposals': list(analysis_results['comparison']['removed_proposals']),
            'total_functions_removed': analysis_results['comparison']['total_functions_removed'],
            'function_reductions_count': len(analysis_results['comparison']['function_reductions'])
        },
        'validation_quality': analysis_results['quality']
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {output_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ” content_validated vs merged_only æ¯”è¼ƒé–‹å§‹")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    merged_only_path = "../data/ground_truth/accepted_proposals_ground_truth_merged_only.json"
    content_validated_path = "../data/ground_truth/accepted_proposals_ground_truth_content_validated.json"
    
    print(f"ğŸ“ merged_only: {merged_only_path}")
    print(f"ğŸ“ content_validated: {content_validated_path}")
    
    merged_only_data = load_ground_truth_data(merged_only_path)
    content_validated_data = load_ground_truth_data(content_validated_path)
    
    if not merged_only_data or not content_validated_data:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # å„ç‰ˆã®è©³ç´°åˆ†æ
    merged_only_analysis = analyze_ground_truth_quality(merged_only_data, "merged_only")
    content_validated_analysis = analyze_ground_truth_quality(content_validated_data, "content_validated")
    
    # å¤‰æ›´å†…å®¹æ¤œè¨¼ã®å½±éŸ¿åˆ†æ
    comparison_results = compare_ground_truth_impact(merged_only_analysis, content_validated_analysis)
    
    # æ¤œè¨¼å“è³ªã®è©³ç´°åˆ†æ
    quality_analysis = analyze_validation_quality(content_validated_analysis)
    
    # çµæœã‚’ã¾ã¨ã‚ã¦ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    analysis_results = {
        'merged_only': merged_only_analysis,
        'content_validated': content_validated_analysis,
        'comparison': comparison_results,
        'quality': quality_analysis
    }
    
    output_path = "../data/ground_truth/content_validation_impact_report.json"
    save_comparison_report(analysis_results, output_path)
    
    print(f"\nâœ… å¤‰æ›´å†…å®¹æ¤œè¨¼ã®å½±éŸ¿åˆ†æå®Œäº†ï¼")

if __name__ == "__main__":
    main()
