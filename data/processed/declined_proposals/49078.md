=== Fetching Proposal: Limited use cases ===
Issue URL: https://github.com/golang/go/issues/49078

==== [Issue Title] ====
proposal: database/sql: Add MaxRepreparedStatements and MaxRetries to DBStats

==== [Issue Body] ====
This is a proposal to add 2 new fields to the [DBStats](https://pkg.go.dev/database/sql#DBStats) struct:

    // Counters
    // ...
    MaxRetries              int64 // The total number of operation retries
    MaxRepreparedStatements int64 // The total number of times a prepared statement has been reprepared due to not existing on the chosen connection

Implementation would be with `atomic.AddInt64` (similar to [this PR](https://go-review.googlesource.com/c/go/+/322049) for another new `DBStat`). I'm happy to handle implementation here.

# MaxRepreparedStatements

An `int64` field that exposes the cumulative number of times a prepared statement was executed but the connection used did not already have the statement prepared on it so it had to re-prepare it. [This happens here](https://github.com/golang/go/blob/3befaf0cdb18420f45acfa7cee725297aa550faf/src/database/sql/sql.go#L2697-L2700) in `database/sql`.

## Why?

Prepared statements offer a performance benefit since the server can use more efficient protocols and/or skip re-analyzing and planning the query ([PostgreSQL docs](https://www.postgresql.org/docs/10/sql-prepare.html#:~:text=A%20prepared%20statement%20is%20a%20server-side%20object%20that%20can%20be%20used%20to%20optimize%20performance.), [MySQL docs](https://dev.mysql.com/doc/refman/8.0/en/sql-prepared-statements.html#:~:text=Less%20overhead%20for%20parsing%20the%20statement%20each%20time%20it%20is%20executed.%20Typically%2C%20database%20applications%20process%20large%20volumes%20of%20almost-identical%20statements%2C%20with%20only%20changes%20to%20literal%20or%20variable%20values%20in%20clauses%20such%20as%20WHERE%20for%20queries%20and%20deletes%2C%20SET%20for%20updates%2C%20and%20VALUES%20for%20inserts.)). This benefit can be lost though if the statement needs to be reprepared before being executed (as it’s another network roundtrip and the server will potentially need to replan and analyze the query).

This metric would be useful to monitor to see if it correlates with increased latencies. It’s also useful in finding the optimal values for settings like [MaxConnLifetime](https://pkg.go.dev/database/sql#DB.SetConnMaxLifetime). If `MaxRepreparedStatements` is constantly increasing it might mean you need to increase the lifespan of your connections so statements don’t need to be reprepared on new connections.

## What about using server metrics for this?

Some SQL databases expose a counter on how often `PREPARE` is called (e.g. [`Com_stmt_prepare` in MySQL](https://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html#:~:text=are%20as%20follows%3A-,Com_stmt_prepare,-Com_stmt_execute)). While this is useful, it does not cover the same use case as a driver metric. Driver metrics can be more specific (since each service might have their own driver instance you could attach that as a metric tag). `Com_stmt_prepare` can also increase in cases where `MaxRepreparedStatements` doesn’t: if a driver is used in only thread, prepares a statement, executes it 5 times synchronously, and then closes the statement then `Com_stmt_prepare` will increase on the database but `MaxRepreparedStatements` shouldn’t increase in the driver.

# MaxRetries

An `int64` field that exposes the cumulative number of times an operation has been retried. For example, when calling `ExecContext` if the initial attempt fails with `ErrBadConn` then `database/sql` [will retry once with a cached or new connection and then once more with a new connection](https://github.com/golang/go/blob/07e5527249cb0b152a3807d67ea83bafd71d2496/src/database/sql/sql.go#L1600-L1608). Each of these retries would increment the field by 1.

## Why?

This metric is useful for determining the source of tail latencies. Retries can lead to having to establish an entirely new connection which has overhead. This would be a useful metric to monitor to see if it’s correlating with increased latencies. It could indicate networking issues or other problems connecting and sending data to the DB.



==== [Comments] ====

--- Comment #1 by ianlancetaylor ---
CC @kardianos 

--- Comment #2 by rsc ---
This seems incredibly specific. Do we really need to add it to the standard package?
Could a custom driver watch for this kind of thing?
Or is there some better 'back ends might not be healthy' metric?


--- Comment #3 by rsc ---

This proposal has been added to the [active column](https://golang.org/s/proposal-status#active) of the proposals project
and will now be reviewed at the weekly proposal review meetings.
— rsc for the proposal review group


--- Comment #4 by rsc ---
ping @kardianos. Still not sure this is the right approach for the general problem of detecting unhealthy backends.


--- Comment #5 by kardianos ---
I don't think this would help much. Knowing the current DB connection size either from the client or server and comparing them to the tail latency would probably be more useful. In a sql/v2, retry behavior/implementation would be something I would want to be able to swap out, but I don't think adding this statistic wold be worthwhile.

Possibly one proxy you could use instead is to look at new connections by creating a Connector wrapper. If a DB restarts and all the connections close and must be "retried", then you would see many new DB connections being made.

I would suggest "decline".

--- Comment #6 by rsc ---

Based on the discussion above, this proposal seems like a **[likely decline](https://golang.org/s/proposal-status#likely-decline)**.
— rsc for the proposal review group


--- Comment #7 by rsc ---

No change in consensus, so **[declined](https://golang.org/s/proposal-status#declined)**.
— rsc for the proposal review group

