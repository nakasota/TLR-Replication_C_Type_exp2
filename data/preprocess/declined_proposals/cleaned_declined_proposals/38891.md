==== [Issue Title] ====
proposal: sync: Add UnlockToRLock() to RWMutex

==== [Issue Body] ====
There is a scenario that can't be done efficiently (or at all) from outside sync package.
This is with regards to **RWMutex**

If you have an operation that has a well delineated "write" portion and "read" portion:

1. Aquire a **Lock**
2. Do something with the lock.
3. Release the **Lock**.
4. Immediately acquire **RLock**.
5. Do something with read lock.
6. Release the **RUnlock**.

The objective is:

1. Increase throughput by allowing more goroutines wishing to Read to be able to do so. **AND**
2. Prevent another goroutine that wants to Lock from interjecting between **steps 3 and 4** above.

My proposal is a function called **`UnlockToRLock()`** that is simple to implement from inside `sync` package that covers the scenario above.

The alternative is:

1. Reduce throughput by keeping Step 1-6 inside a **Lock** OR
2. ~~Accept a goroutine that wants to acquire lock interjecting between Step 3&4~~ (the scenario assumes an operation where the "read" portion must follow the "write" portion, and it is not correct for another goroutine to potentially interfere before the "read")

==== [Comments] ====

--- Comment #1 by ianlancetaylor ---
I think this is a dup of #4026 and #23513.

--- Comment #2 by navytux ---
For the reference: pygolang settled on `UnlockToRLock` name:

https://lab.nexedi.com/nexedi/pygolang/commit/a9345a98



--- Comment #3 by rsc ---
Over on https://github.com/golang/go/issues/23513#issuecomment-360474661 I said:

> I'm not convinced this is helpful in a real program, and there is a real cost to adding complexity to the API. Please show us benchmarks improving a real workload, not a microbenchmark.

Optimizing the "holding for writing and dropping to holding for reading" makes it sound like "holding for writing" is something that can be done efficiently, but my understanding is that on today's machines, if an RWMutex is held for writing with _any_ appreciable frequency, you don't see performance wins beyond having a plain Mutex. The contention from the writers dominates everything else.

If my understanding is correct, then this new API surface would add complexity without any performance benefit, and so we shouldn't add it.

Does anyone have numbers to address this concern, one way or the other?


--- Comment #4 by networkimprov ---
@pjebs, you could get some of the semantic benefit of UnlockAndRLock() with TryRLock(). Any thoughts?

--- Comment #5 by navytux ---
@rsc, on my side it is not the performance in the first place, but semantic guarantee of not releasing the lock while downgrading it. If there is no such guarantee, the writer that  done modification part of its job and no longer needs the lock to be exclusively held, needs to **recheck the data for consistency after Unlock->Rlock** because the data might have been changed by other mutators while the lock was not held. With atomic downgrade there is no such need.

And contrary to upgrade, which is not possible to implement without deadlocks, atomic downgrade is well-defined operation with well-defined semantic.

Leaving performance topic for others.

--- Comment #6 by ianlancetaylor ---
@navytux The question that @rsc is asking is: if you need to downgrade the lock, would you get better performance overall by always using a `sync.Mutex` rather than a `sync.RWMutex`.  `sync.RWMutex` is more complex and harder to understand than `sync.Mutex`, so the only reason to ever use a `sync.RWMutex` is performance.  If you need to acquire a write lock and downgrade the lock, @rsc is suggesting that you will get a better performance by using `sync.Mutex`.  There is no reason to make `sync.RWMutex` still more complex if it doesn't give a performance advantage.

--- Comment #7 by networkimprov ---
There are other reasons to use a RWMutex, including to wait for all potentially-concurrent tasks to cease (and block new ones), e.g. before shutdown/upgrade. Such tasks acquire a long-lived read lock.

If a situation implies a RWMutex, you should use that, even if there's no apparent performance issue. As the app evolves, or its workloads in the field increase, you avoid the need to replace a Mutex with a RWMutex. And you have more meaningful code.

You might also be able to replace a collection of Mutexes covering separate resources with a single RWMutex covering them all, if most of the access to them is read-only.

--- Comment #8 by ianlancetaylor ---
@networkimprov Those are nice examples, but do any of them require a `UnlockThenRLock` method?

--- Comment #9 by as ---
I experimented with this concept here https://github.com/as/lock/blob/master/lock.go and it was called `Downgrade`.

In practice, it had very little benefit for similar reasons that @rsc mentioned.

--- Comment #10 by networkimprov ---
My point is that performance is really not the correct focus of this discussion.

@navytux makes a good point re the need to recheck the data for consistency after Unlock->RLock. That operation may not be lightweight. 

--- Comment #11 by as ---
> My point is that performance is really not the correct focus of this discussion.

@networkimprov I disagree, the consistency can only be violated if you release the lock. Why do you release the lock and re-acquire the read lock?

--- Comment #12 by ianlancetaylor ---
Performance really does come in, because the code can simply replace the `sync.RWMutex` with a `sync.Mutex`.  The resulting code will be simpler, and there will be no need to use `UnlockThenRLock` because you will always hold a full lock.  And it seems that for cases where it is useful to call `UnlockThenRLock`, using a `sync.Mutex` may well be faster.  So the question is: if it would be both faster and simpler to use a `sync.Mutex`, then why should we make the implementation of `sync.RWMutex` more complex?

--- Comment #13 by as ---
The temptation that leads to these types of proposals is that there are many readers and few writers. The writer is finished writing in the critical section and wants to unblock the readers, but for some reason, needs read access in the critical section.

However, there is usually a way to avoid reading in the critical section altogether. For example, a writer stores a value into a map, and then loads it and returns it. There is no reason to really load the element since the writer was the one who put it in there.

In more complex cases, this is desired when there are multiple maps, but the result often has negligible performance benefits upon systematic benchmarking. I suspect this is because it is easy to underestimate the complexity of `sync.RWMutex` and overestimate the penalty of doing extra operations while holding the write side of the lock in the critical section.

--- Comment #14 by networkimprov ---
I think we should let @pjebs and @navytux explain their use cases before we claim that they're weak :-)

I'm disappointed to see the semantic value of RWMutex brushed off. Switching to a Mutex for this situation may make the code "simpler" in a way, but ultimately less meaningful.

This is also the third (at least) request for this feature, so maybe there's something to it.

--- Comment #15 by as ---
@networkimprov I found these, but the first one is talking about upgrading a RWMutex reader to writer. Do you have the issue for the third request?

https://github.com/golang/go/issues/4026
https://github.com/golang/go/issues/23513#issue-290637132
https://github.com/golang/go/issues/38891


--- Comment #16 by navytux ---
> @navytux The question that @rsc is asking is: if you need to downgrade the lock, would you get better performance overall by always using a sync.Mutex rather than a sync.RWMutex.

@ianlancetaylor thanks for clarifying the context of this question. In short my answer is "no" - RWMutex - also known as shared readers / exclusive writer lock - brings real performance benefit **because it allows many readers to be run simultaneously** instead of running only a single reader at a time as would be implied by regular sync.Mutex.

Now let me try to explain:

In the filesystem I write, both at server and at client sides, a particular database view is maintained for transactional data snapshot. There are readers of the data **which need data snapshot to remain stable throughout their operations** and tasks that update database view (e.g. as new transactions are being committed and database sends corresponding notifications). Here it is natural to use shared-readers/single-writer lock - aka sync.RWMutex - to organize synchronization in between readers and snapshot updater. However contrary to usual scenarios with plain mutex **the lock is _not_ short-lived** because readers perform network round-trip while accessing the database **under the lock** for data snapshot to remain stable. I think in this case it goes without saying that using `sync.Mutex` instead of `sync.RWMutex` brings significant performance degradation because then there would be only 1 allowed simultaneous database user.

The above could work without atomic downgrade if the updater could work with only taking the lock for write and then completely releasing it when done. That is however not the case for me:

- on client, after first part of the update process, the updater needs to let other "snapshot observer" (pinner thread) to run, because second part of the update have to send requests to filesystem server that trigger messages to the pinner that _must_ be handled and replied, or else the whole thing will get stuck due to design of virtual memory handling.
- `Unlock`'ing and then `RLock`'ing separately can cause deadlocks, because in the presence of multiple shared locks, if it was e.g. originally `A.W B.R` lock order, without atomic downgrade and doing `A.Unlock + A.RLock` it will become `B.R A.R` leading to AB-BA style deadlock wrt another process which tries to take `A.R B.R`.
- it is indeed not trivial to recheck potentially changed state if snapshot lock was released and retaken because there can be another simultaneous resync requests (= requests to change database view) initiated by client.

Organization of client locking: https://lab.nexedi.com/kirr/wendelin.core/blob/9f5f8bab/wcfs/client/wcfs.cpp#L117-184
The place where client.resync needs atomic downgrade:
https://lab.nexedi.com/kirr/wendelin.core/blob/9f5f8bab/wcfs/client/wcfs.cpp#L716-756
Organization of locking on filesystem server:
https://lab.nexedi.com/kirr/wendelin.core/blob/9f5f8bab/wcfs/wcfs.go#L428-464


Overall I think for a well-defined and well-understood operation it is better to put complexity into sync.RWMutex instead of exposing complexity to programmers and forcing them to solve this tasks by themselves many times instead of only once, properly and for everyone.

For naming my rationale for `UnlockToRLock` is that `UnlockThenRLock` name does not tell a reader that operation is atomic, and it could be perceived as real `Unlock` then `RLock`. On the other hand `UnlockToRLock` clearly tells that the mutex is not reaching "unlocked" state in between.

Kirill

--- Comment #17 by networkimprov ---
#4026 asked for both downgrade & upgrade, and it was clarified that upgrade isn't possible without a failure mode.

--- Comment #18 by bcmills ---
The performance argument is interesting to me, because on the one hand it seems like a `RWLock` should be mostly harmless if the writer-locked sections are very short, but on the other hand the very fairness of `RWLock` makes even a very short writer-locked section dangerous: it is possible for a reader lock to block the writer for a long time, which then blocks all _other_ readers behind that writer. So even a `RWLock` with a very short critical section for the writer can be quite dangerous.

(And, to be honest, most of the uses of `RWLock` that I've seen would be better-served by some other synchronization pattern anyway...)

--- Comment #19 by bcmills ---
@navytux, note that for “snapshot”-style uses in particular, an `atomic.Value` is almost always a better fit, perhaps in conjunction with a `sync.Mutex` to restrict concurrent updates. With that approach, the readers never block.

--- Comment #20 by networkimprov ---
> it is possible for a reader lock to block the writer for a long time, which then blocks all other readers 

Bryan, how is that worse than a Mutex in the scenario you give, or are you saying that any Mutex is wrong for it?

> most ... would be better-served by some other synchronization pattern 

For example?

--- Comment #21 by as ---
@networkimprov RCU

--- Comment #22 by cespare ---
> (And, to be honest, most of the uses of RWLock that I've seen would be better-served by some other synchronization pattern anyway...)

I endorse this. I think we should avoid adding features to rwmutexes that make them more tempting to use than they already are.

I like [@nelhage's blog post](https://blog.nelhage.com/post/rwlock-contention/) as a reference for the problems with read-write locks.

--- Comment #23 by navytux ---
> @navytux, note that for “snapshot”-style uses in particular, an atomic.Value is almost always a better fit, perhaps in conjunction with a sync.Mutex to restrict concurrent updates.

@bcmills, by "snapshot" I was meaning something different than just one value snapshot. In my case it is something like database connection with many objects obtained through it and exposed to the user. When updating we need to update both the connection _and_ already exposed objects in consistent way. Maybe I'm missing something, but it is hard for me to see how `atomic.Value` helps here. The write path - the updater - is also not "small" as it involves network round-trips too. I still stand on my point that shared/exclusive lock is the best fit to this scenario. Please see details in my original [post](https://github.com/golang/go/issues/38891#issuecomment-632033192).

The part of the writer that performs network round-trip:
https://lab.nexedi.com/kirr/wendelin.core/blob/9f5f8bab/wcfs/client/wcfs.cpp#L733-753

--- Comment #24 by navytux ---
It is true that whether shared-lock is writer-priority, or reader-priority, or of some other kind, makes a difference. Even more: wcfs implements additional custom locking protocol on top of writer-priotity sync.RWMutex to avoid deadlocks due to potential locking cycle through page lock in OS kernel:

https://lab.nexedi.com/kirr/wendelin.core/blob/7dbddf9d/wcfs/notes.txt#L186-205
https://lab.nexedi.com/kirr/wendelin.core/blob/7dbddf9d/wcfs/wcfs.go#L547-565

That does not mean shared locks are useless.

It is true that RWMutex locking patterns can be implemented with just channels (see e.g. [here](https://lab.nexedi.com/nexedi/pygolang/blob/c3fdd1c0/golang/sync.h#L105-136) ([2](https://lab.nexedi.com/nexedi/pygolang/blob/c3fdd1c0/golang/sync.cpp#L29-129)) for writer-preference version). It is also however true that just plain `sync.Mutex` can be implemented with just    channel as well.

Does that mean that Mutex and/or RWMutex are useless and should not be there?
My personal answer is no.

--- Comment #25 by networkimprov ---
So Caleb, Bryan, and "as" suggest that one must roll your own sync scheme with atomics when UnlockToRLock() is required, because a RWMutex isn't optimal in certain (possibly unrelated) scenarios?

Are there any widely used, well-tested third-party packages with alternate sync schemes?

Roll-your-own sync may not turn out well. Isn't safeguarding Go users against incorrect code a higher priority than directing them towards optimal code? (And isn't there a proverb about premature optimization? :-)

--- Comment #26 by bcmills ---
@networkimprov, converting a write-lock to a read-lock is never “required”: you can keep holding the write-lock instead. Our argument is that if you _need_ an optimization, `UnlockThenRLock` is usually not the right fix anyway, because you'll still end up with much of the same latency and contention that you would have had just holding the write lock.

--- Comment #27 by rsc ---
@bcmills's last comment makes clear why I brought up performance: if there's not a performance win, then just keep holding the write lock.

@navytux's scenario is fascinating. I am honestly a bit surprised that you've gotten as far as you have without wanting control over the read-write contention policy. I'd have expected that a database would care about exactly how to resolve those conflicts and not be at the mercy of whatever the RWMutex implementation does, especially since the RWLock is tuned for in-memory protection (with very different concerns than a database).

Thanks @as for reporting back on past experience with this operation (and that it didn't help much).

Overall, the general sentiment seems to be that we can do without adding this operation, except for @navytux's example. Am I reading that right?


--- Comment #28 by navytux ---
@rsc, thanks for feedback and kind words. Writer-preference shared/exclusive lock semantic fits into my need for contention policy, that why I'm using sync.RWMutex directly.

While I understand that majority of the uses for `sync.Mutex` and `sync.RWMutex` are for short-lived critical sections, _nothing_ in [sync package documentation](https://tip.golang.org/pkg/sync) says so. There it is only said that those primitives provide *mutual exclusion* of various kinds. And as e.g. [my example](https://github.com/golang/go/issues/38891#issuecomment-632033192) shows, mutual exclusion is not always used for short-lived interaction. Talking about sync.Mutex, there is even a sign that [it is not always used as a kind of critical section lock](https://tip.golang.org/pkg/sync/#Mutex.Unlock) because it implements semantic of 1 valued semaphore which *can be released by actor different from who originally acquired it*. This property is not an overlook - for example it was [explicitly preserved](https://github.com/golang/go/issues/9201) when @dvyukov proposed to remove it for sync.Mutex semantic to become more akin to critical section lock and thus more tractable for race/deadlock detector.

What I'm trying to say is: if `sync.*Mutex` provide "mutual exclusion", not "critical sections", why are we bringing arguments that apply only to "critical sections" cases? In other words: why are we ruling out arguments coming from cases where locked mutex states are long-lived?

From what I've read above, my understanding is that it is very likely for my case to be declared "special", `RWMutex.UnlockToRLock` rejected - similarly to the fate of https://github.com/golang/go/issues/4026, https://github.com/golang/go/issues/23513, ["Atomically downgrade write lock to read lock?"](https://groups.google.com/forum/#!topic/golang-nuts/MmIDUzl8HA0) thread on golang-nuts, etc ...; and that I should go and implement high-level synchronization primitives myself, or maybe even ["substitute a custom RWMutex implementation into your own code (copy and modify)"](https://github.com/golang/go/issues/23513#issuecomment-361395248).

While certainly doable, I think that would be a pity overall outcome.

Kirill

/cc @nixprime, @dsymonds, @josharian 

--- Comment #29 by bcmills ---
@navytux, I think you're reading more into the phrase “critical section” than at least I intended. I interpret the “critical section” as a contiguous segment of the program's _execution trace_, not its _source code_. (That is: the “critical section” is the portion of the execution of the program during which the lock is held.)

If you don't like that term, the same argument applies to “mutual exclusion”: as far as I can tell `sync.RWMutex` is prone to latency spikes whenever it may be read- _or_ write-locked for a long duration, regardless of how that duration is divided among goroutines.

--- Comment #30 by rsc ---
Yes, in my mind a "critical section" can begin in one goroutine and be finished in another goroutine. That's why sync.Mutex allows unlocking in a different goroutine.
